const { defineSecret } = require('firebase-functions/params');
const { VertexAI } = require('@google-cloud/vertexai');

const GOOGLE_AI_STUDIO_API_KEY = defineSecret('GOOGLEAISTUDIOAPIKEY');

/**
 * Calls the LLM (Gemini) with the given system and user prompts.
 * Returns the text response.
 */
async function callLLM(systemPrompt, userPrompt, modelName = 'gemini-1.5-flash') {
    // Note: In a real Firebase Functions environment, we might use the Vertex AI SDK 
    // or the Google AI Studio REST API directly depending on the setup.
    // The existing codebase seems to use a custom implementation or Vertex AI.
    // For this implementation, we will use a standard fetch approach if the SDK isn't fully configured,
    // or use the VertexAI SDK if available.

    // Let's check if we can reuse the logic from index.js, but since we are making a clean util:

    const apiKey = GOOGLE_AI_STUDIO_API_KEY.value();
    if (!apiKey) {
        throw new Error('GOOGLE_AI_STUDIO_API_KEY is not set');
    }

    const url = `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${apiKey}`;

    const payload = {
        contents: [{
            role: 'user',
            parts: [{ text: `${systemPrompt}\n\n${userPrompt}` }]
        }],
        generationConfig: {
            temperature: 0.2,
            maxOutputTokens: 2048,
        }
    };

    try {
        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(payload)
        });

        if (!response.ok) {
            const errorText = await response.text();
            throw new Error(`LLM API Error: ${response.status} - ${errorText}`);
        }

        const data = await response.json();
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text;

        if (!text) {
            throw new Error('No content generated by LLM');
        }

        // Clean up markdown code blocks if present (common with JSON responses)
        const cleaned = text.replace(/```json\n?|\n?```/g, '').trim();
        return cleaned;

    } catch (error) {
        console.error('LLM Call Failed:', error);
        throw error;
    }
}

module.exports = { callLLM };
